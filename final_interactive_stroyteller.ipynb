{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Installing the neccesary dependencies"
      ],
      "metadata": {
        "id": "St9pPLcWBroM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS moviepy diffusers"
      ],
      "metadata": {
        "id": "YZxrgWV9Bc60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install diffusers[\"torch\"] transformers\n",
        "!pip install accelerate\n",
        "!pip install git+https://github.com/huggingface/diffusers"
      ],
      "metadata": {
        "id": "d_23PLeRBj4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install intel_extension_for_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoT31-R2KDM9",
        "outputId": "b407ea7e-adc4-477b-e7b5-3e35f346e5f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Story Generation"
      ],
      "metadata": {
        "id": "X1IZoJ5K_nw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "import intel_extension_for_pytorch as ipex\n",
        "# Set up the pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Define the text prompt\n",
        "text = \" a boy in the forest\"\n",
        "prompt = \"generate a story on the title\" + text\n",
        "\n",
        "# Set the maximum number of tokens\n",
        "max_length = 1024\n",
        "\n",
        "# Generate the story\n",
        "story = pipe(\n",
        "    prompt,\n",
        "    max_length=max_length,\n",
        "    truncation=True\n",
        ")[0]['generated_text'][len(prompt)+2:]\n",
        "print(story)"
      ],
      "metadata": {
        "id": "UzNGUB69_q0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Audio Generation"
      ],
      "metadata": {
        "id": "8-UmtyLi_z4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "narration_text = story\n",
        "narration = gTTS(text=narration_text, lang='en-us', slow=True, tld='com')\n",
        "narration.save(\"narration.mp3\")\n",
        "audio = AudioFileClip(\"narration.mp3\")\n",
        "duration = audio.duration"
      ],
      "metadata": {
        "id": "K-a3ANaW_8a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image Generation"
      ],
      "metadata": {
        "id": "j-gYwm7jACl_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P08h4uQEP76I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "# Load the model\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"digiplay/majicMIX_realistic_v6\", torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "pipe.safety_checker = None\n",
        "\n",
        "h = 800  # height of the image\n",
        "w = 640  # width of the image\n",
        "steps = 25  # number of updates the system makes before giving the result, making it more accurate\n",
        "guidance = 7.5  # how closely you want the image to be related to the prompt that you have typed\n",
        "neg = \"easynegative,no repetation, lowres,partial view, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worstquality, low quality, normal quality, jpegartifacts, signature, watermark, username, blurry, bad feet, cropped, poorly drawn hands, poorly drawn face, mutation, deformed, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, extra fingers, fewer digits, extra limbs, extra arms,extra legs, malformed limbs, fused fingers, too many fingers, long neck, cross-eyed,mutated hands, polar lowres, bad body, bad proportions, gross proportions, text, error, missing fingers, missing arms, missing legs, extra digit, extra arms, extra leg, extra foot,\"\n",
        "\n",
        "prompt=text\n",
        "num_images=int(duration/3)\n",
        "for i in range(num_images):\n",
        "    prompt = prompt\n",
        "    image = pipe(prompt, height=h, width=w, number_inference_steps=steps, guidance_scale=guidance, negative_prompt=neg).images[0]\n",
        "    image.save(f\"image_{i+1}.png\")  # Save the image with a unique name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Video Concatenation"
      ],
      "metadata": {
        "id": "y8n4ulu2BO86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import ImageClip\n",
        "from moviepy.editor import AudioFileClip\n",
        "from moviepy.editor import ColorClip,CompositeVideoClip,concatenate_videoclips\n",
        "# Create a blank video clip with the same duration as the audio\n",
        "video = ColorClip((1280, 720), color=(0, 0, 0), duration=duration)\n",
        "\n",
        "# Set the text for the video\n",
        "txt = story\n",
        "\n",
        "# Add the text clip to the video\n",
        "video = video.set_audio(audio)\n",
        "video = video.set_duration(duration)\n",
        "video = video.set_fps(24)\n",
        "video = video.set_audio(audio)\n",
        "video = video.set_duration(duration)\n",
        "video = video.set_fps(24)\n",
        "video = CompositeVideoClip([video])\n",
        "\n",
        "# Add images to the video\n",
        "image_clips = [ImageClip(f\"/content/image_{i}.png\").set_duration(duration/num_images) for i in range(1, num_images+1)]\n",
        "\n",
        "# Create a video from the images\n",
        "images_video = concatenate_videoclips(image_clips, method=\"compose\")\n",
        "\n",
        "# Overlay the images video on top of the main video\n",
        "final_video = CompositeVideoClip([video.set_position(('center', 'center')), images_video.set_position(('center', 'center'))])\n",
        "\n",
        "# Write the final video to a file\n",
        "final_video.write_videofile(\"story_video2.mp4\", codec='libx264', fps=24)"
      ],
      "metadata": {
        "id": "Q-_wDmGuAKIy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}